# Chatbot Architecture Redesign

## Implementation Status

### âœ… Completed (2025-10-21)

**Phase 1: Pandas-Based Aggregation** - FULLY IMPLEMENTED
- âœ… Removed AUTO mode from aggregate_data
- âœ… Implemented flatten_data tool
- âœ… Implemented dataframe_query tool
- âœ… Updated system prompt with mandatory workflows
- âœ… Comprehensive documentation

See `IMPLEMENTATION_SUMMARY.md` for details.

### ğŸ”„ In Progress

None currently.

### â³ TODO

**Phase 2: Fulltext Search Tool** - NOT STARTED
- â³ Implement Whoosh-based search_in_data tool
- â³ Update system prompt

**Phase 3: Truncation Policy** - NOT STARTED
- â³ Review SmartTool max_tokens behavior
- â³ Ensure cache-first, never truncate

**Phase 4: Integration Tests** - NOT STARTED
- â³ Create test suite for each tool
- â³ Reproduce LLM usage patterns

---

## Current Problems

### 1. inspect_api_structure Rarely Used
**Issue**: Agent doesn't inspect structure before fetching large data
**Impact**: Fetches all fields when only few are needed

### 2. AUTO Mode Overused
**Issue**: aggregate_data defaults to AUTO, agent doesn't think about what fields are needed
**Impact**: Returns fields user didn't ask for, wastes tokens

### 3. Limited Aggregation Capabilities
**Issue**: Current aggregate_data only does basic operations
**Impact**: Can't filter, sort, or do complex pandas operations

### 4. Fulltext Search Inefficient
**Issue**: Large API responses sent to LLM for text search
**Impact**: Token waste, slow, error-prone

### 5. Token Truncation Issues
**Issue**: Tools truncate data when exceeding max_tokens
**Impact**: Data loss, incomplete results

### 6. Tool Usage Errors
**Issue**: LLM doesn't have enough examples/validation for complex tools
**Impact**: Frequent parameter errors, failed tool calls

---

## Proposed New Architecture

### Phase 1: Pandas-Based Aggregation Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. INSPECT (mandatory for large responses)                 â”‚
â”‚                                                              â”‚
â”‚  inspect_api_structure(api_type="dataset", dataset_name=...)â”‚
â”‚  â†’ Returns: Available fields, types, samples                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. FETCH with Field Selection                              â”‚
â”‚                                                              â”‚
â”‚  get_dataset_entries(                                       â”‚
â”‚      dataset_name=...,                                      â”‚
â”‚      fields=[...],  # Based on inspection                  â”‚
â”‚      raw_filter=... # OData filter if possible             â”‚
â”‚  )                                                           â”‚
â”‚  â†’ Returns: cache_key (if large)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. FLATTEN (convert nested JSON to tabular)                â”‚
â”‚                                                              â”‚
â”‚  flatten_data(                                              â”‚
â”‚      cache_key="...",                                       â”‚
â”‚      max_depth=2,                                           â”‚
â”‚      array_handling="explode"  # or "stringify"            â”‚
â”‚  )                                                           â”‚
â”‚  â†’ Returns: Flattened data, stores as DataFrame            â”‚
â”‚  â†’ New cache_key: "df_<id>"                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. PANDAS OPERATIONS (on cached DataFrame)                 â”‚
â”‚                                                              â”‚
â”‚  dataframe_query(                                           â”‚
â”‚      df_key="df_<id>",                                     â”‚
â”‚      operation="filter",                                    â”‚
â”‚      query="Shortname.str.contains('Hotel')",              â”‚
â”‚  )                                                           â”‚
â”‚                                                              â”‚
â”‚  dataframe_query(                                           â”‚
â”‚      df_key="df_<id>",                                     â”‚
â”‚      operation="sort",                                      â”‚
â”‚      by="Shortname",                                        â”‚
â”‚      ascending=True                                         â”‚
â”‚  )                                                           â”‚
â”‚                                                              â”‚
â”‚  dataframe_query(                                           â”‚
â”‚      df_key="df_<id>",                                     â”‚
â”‚      operation="groupby",                                   â”‚
â”‚      by="Dataspace",                                        â”‚
â”‚      agg={"Shortname": "count"}                            â”‚
â”‚  )                                                           â”‚
â”‚  â†’ Each returns new df_key or final result                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Fulltext Search Tool

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FULLTEXT SEARCH (Whoosh-based)                             â”‚
â”‚                                                              â”‚
â”‚  search_in_data(                                            â”‚
â”‚      cache_key="...",                                       â”‚
â”‚      query="hotel mountains",                               â”‚
â”‚      fields=["Shortname", "ApiDescription"],               â”‚
â”‚      limit=20                                               â”‚
â”‚  )                                                           â”‚
â”‚                                                              â”‚
â”‚  Flow:                                                       â”‚
â”‚  1. Load data from cache                                    â”‚
â”‚  2. Index specified fields with Whoosh                     â”‚
â”‚  3. Execute search query                                    â”‚
â”‚  4. Return matching items only                              â”‚
â”‚                                                              â”‚
â”‚  Benefits:                                                   â”‚
â”‚  - No LLM needed for text search                           â”‚
â”‚  - Fast, efficient                                          â”‚
â”‚  - Returns only matches (not all data)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 3: No Truncation Policy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOOL OUTPUT POLICY                                         â”‚
â”‚                                                              â”‚
â”‚  IF result_size > max_tokens:                              â”‚
â”‚      cache_key = cache.store(result)                       â”‚
â”‚      return {                                               â”‚
â”‚          "cached": true,                                    â”‚
â”‚          "cache_key": cache_key,                           â”‚
â”‚          "size_info": {                                     â”‚
â”‚              "total_items": N,                             â”‚
â”‚              "estimated_tokens": X                          â”‚
â”‚          },                                                  â”‚
â”‚          "sample": result[:5],                             â”‚
â”‚          "next_steps": [                                    â”‚
â”‚              "Use dataframe_query to filter",              â”‚
â”‚              "Use search_in_data for text search",         â”‚
â”‚              "Use flatten_data for tabular view"           â”‚
â”‚          ]                                                   â”‚
â”‚      }                                                       â”‚
â”‚  ELSE:                                                       â”‚
â”‚      return result directly                                 â”‚
â”‚                                                              â”‚
â”‚  NEVER truncate or emergency_summarize                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## New Tool Specifications

### 1. inspect_api_structure (Enhanced)

**When to use**: MANDATORY before any large fetch

**System Prompt Rule**:
```
BEFORE calling get_datasets(aggregation_level="full") or get_dataset_entries:
  IF you don't know what fields exist:
    CALL inspect_api_structure first
    ANALYZE the fields
    THEN fetch with specific fields parameter
```

**Returns**:
```json
{
  "fields": [
    {"path": "Shortname", "type": "string", "sample": "Hotel ABC"},
    {"path": "GpsInfo.Latitude", "type": "number", "sample": 46.4983}
  ],
  "field_count": 47,
  "total_records": 8532,
  "recommended_fields": {
    "minimal": ["Id", "Shortname"],
    "standard": ["Id", "Shortname", "Type", "GpsInfo"],
    "detailed": ["Id", "Shortname", "Type", "GpsInfo", "ContactInfos", "ApiDescription"]
  }
}
```

### 2. flatten_data (NEW)

**Purpose**: Convert nested JSON to flat tabular structure

**Parameters**:
```python
cache_key: str  # Data to flatten
max_depth: int = 2  # How deep to flatten
array_handling: "explode" | "stringify" = "explode"
columns: list[str] | None = None  # Specific columns to include
```

**Example**:
```python
# Input (nested):
{
  "Shortname": "Hotel ABC",
  "GpsInfo": {"Latitude": 46.5, "Longitude": 11.3},
  "Tags": ["luxury", "spa"]
}

# Output (flattened):
{
  "Shortname": "Hotel ABC",
  "GpsInfo.Latitude": 46.5,
  "GpsInfo.Longitude": 11.3,
  "Tags": "luxury, spa"  # or exploded to multiple rows
}
```

**Returns**:
```json
{
  "df_key": "df_a1b2c3",
  "shape": [167, 25],
  "columns": ["Shortname", "GpsInfo.Latitude", ...],
  "sample": [<first 5 rows>],
  "memory_usage": "2.3 MB"
}
```

### 3. dataframe_query (NEW - Pandas Interface)

**Purpose**: Powerful pandas operations on cached DataFrames

**Operations**:

#### Filter
```python
dataframe_query(
    df_key="df_a1b2c3",
    operation="filter",
    query="Shortname.str.contains('Hotel') & GpsInfo_Latitude > 46.0"
)
```

#### Sort
```python
dataframe_query(
    df_key="df_a1b2c3",
    operation="sort",
    by=["Dataspace", "Shortname"],
    ascending=[True, True]
)
```

#### Group By
```python
dataframe_query(
    df_key="df_a1b2c3",
    operation="groupby",
    by="Dataspace",
    agg={"Shortname": "count", "GpsInfo_Latitude": "mean"}
)
```

#### Select Columns
```python
dataframe_query(
    df_key="df_a1b2c3",
    operation="select",
    columns=["Shortname", "Dataspace"]
)
```

#### Head/Tail
```python
dataframe_query(
    df_key="df_a1b2c3",
    operation="head",
    n=10
)
```

### 4. search_in_data (NEW - Whoosh Fulltext Search)

**Purpose**: Efficient text search in cached data

**Parameters**:
```python
cache_key: str  # Or df_key
query: str  # Search query
fields: list[str]  # Fields to search in
limit: int = 20
fuzzy: bool = False
```

**Example**:
```python
search_in_data(
    cache_key="datasets_full",
    query="hotel mountain south tyrol",
    fields=["Shortname", "ApiDescription.en"],
    limit=20,
    fuzzy=True
)
```

**Returns**:
```json
{
  "matches": 15,
  "items": [
    {
      "score": 0.95,
      "item": {"Shortname": "Mountain Hotel...", ...},
      "matched_fields": ["Shortname", "ApiDescription.en"],
      "highlights": {
        "Shortname": "<b>Mountain</b> <b>Hotel</b>...",
        "ApiDescription.en": "...in <b>South Tyrol</b>..."
      }
    }
  ]
}
```

---

## Integration Tests Structure

### Test File Structure
```
backend/tests/
â”œâ”€â”€ test_tools_integration.py
â”œâ”€â”€ test_tool_inspect.py
â”œâ”€â”€ test_tool_aggregate.py
â”œâ”€â”€ test_tool_dataframe.py
â”œâ”€â”€ test_tool_search.py
â””â”€â”€ fixtures/
    â”œâ”€â”€ sample_datasets.json
    â”œâ”€â”€ sample_accommodation.json
    â””â”€â”€ expected_outputs/
```

### Example Test Cases

#### Test: inspect_api_structure
```python
async def test_inspect_dataset_structure():
    """LLM should be able to call inspect_api_structure correctly"""

    # Simulate LLM call
    result = await inspect_api_structure(
        api_type="dataset",
        dataset_name="Accommodation"
    )

    # Validate structure
    assert "fields" in result
    assert "field_count" in result
    assert len(result["fields"]) > 0
    assert "recommended_fields" in result

    # Validate LLM can understand output
    assert any(f["path"] == "Shortname" for f in result["fields"])
```

#### Test: aggregate_data with explicit strategy
```python
async def test_aggregate_extract_specific_fields():
    """LLM should specify fields, not rely on AUTO"""

    # Setup
    cache_key = cache.store(sample_datasets)

    # Simulate LLM call - MUST provide fields
    result = await _aggregate_data(
        cache_key=cache_key,
        strategy="extract_fields",
        fields=["Shortname", "Dataspace"]  # Explicit!
    )

    # Validate
    assert result["strategy"] == "extract_fields"
    assert len(result["items"]) == 167
    assert all("Shortname" in item for item in result["items"])
    assert all("Dataspace" in item for item in result["items"])
```

#### Test: dataframe_query filter
```python
async def test_dataframe_filter():
    """LLM should be able to filter DataFrames"""

    # Setup
    df_key = create_test_dataframe()

    # Simulate LLM call
    result = await dataframe_query(
        df_key=df_key,
        operation="filter",
        query="Type == 'Hotel' and GpsInfo_Latitude > 46.0"
    )

    # Validate
    assert "df_key" in result or "items" in result
    assert all(item["Type"] == "Hotel" for item in result["items"])
```

---

## Implementation Priority

### Phase 1 (Critical - Do First)
1. âœ… Remove AUTO mode as default from aggregate_data
2. âœ… Make inspect_api_structure mandatory in system prompt
3. âœ… Implement "no truncation" policy - always cache large results
4. âœ… Create flatten_data tool
5. âœ… Create basic integration tests

### Phase 2 (Important)
1. âœ… Implement dataframe_query with basic operations
2. âœ… Add DataFrame caching
3. âœ… Update tool descriptions with concrete examples
4. âœ… Test all tools with realistic scenarios

### Phase 3 (Enhancement)
1. â¬œ Implement search_in_data with Whoosh
2. â¬œ Add advanced pandas operations
3. â¬œ Performance optimization
4. â¬œ Comprehensive test suite

---

## System Prompt Changes

### New Mandatory Rules

```
## TOOL USAGE RULES

### Rule 1: ALWAYS Inspect Before Large Fetches
BEFORE calling get_datasets(aggregation_level="full") or get_dataset_entries:
  CALL inspect_api_structure first
  ANALYZE what fields are available
  DECIDE which fields you need for the question
  THEN fetch with fields=[specific fields]

### Rule 2: NEVER Use AUTO Mode
For aggregate_data:
  ANALYZE the user question
  DETERMINE what fields are needed
  CALL aggregate_data with strategy="extract_fields" and explicit fields=[]
  DO NOT use strategy="auto"

### Rule 3: Use Pandas for Complex Operations
For filtering, sorting, grouping:
  CALL flatten_data to convert to DataFrame
  CALL dataframe_query with specific operation
  DO NOT try to do this in aggregate_data

### Rule 4: Use Fulltext Search Tool
For text search queries ("find hotels containing 'mountain'"):
  CALL search_in_data, NOT fetch all data and search manually
```

---

## Tool Description Template

Each tool should follow this structure:

```python
tool = SmartTool(
    name="tool_name",
    description="""
    ğŸ”§ TOOL NAME - One Line Purpose

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ“‹ WHEN TO USE
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    - Specific use case 1
    - Specific use case 2

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ“‹ PARAMETERS (Required First)
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    1. param_name (type, REQUIRED)
       - What it does
       - Example: "value"
       - âš ï¸  Common mistake: don't do X

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    âœ… CORRECT EXAMPLES
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    Example 1: Simple case
    tool_name(param1="value", param2="value")

    Example 2: Complex case
    tool_name(param1="value", param2=["a", "b"])

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    âŒ COMMON ERRORS
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    Error 1: tool_name(wrong_param="value")
    Fix: tool_name(correct_param="value")

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ“Š RETURNS
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    {
      "field": "value",
      "next_step": "what to do next"
    }
    """,
    func=_tool_function,
    max_tokens=None  # Never truncate!
)
```

---

## Success Metrics

1. **inspect_api_structure usage**: Should be called in >80% of large fetch scenarios
2. **AUTO mode usage**: Should drop to <10% of aggregate_data calls
3. **Field specificity**: Agent should request avg 3-5 specific fields, not all fields
4. **Search efficiency**: Fulltext searches should not fetch >100 items to LLM
5. **No truncation**: 0% of tool outputs should be truncated
6. **Test coverage**: 100% of tools should have integration tests
7. **Tool errors**: <5% of tool calls should fail due to parameter errors

---

## Migration Plan

1. Create new tools in parallel to existing ones
2. Test extensively with integration tests
3. Update system prompt gradually
4. Monitor agent behavior
5. Phase out old tools once new ones proven
6. Document all changes

---

**Last Updated**: 2025-10-21
**Version**: 2.0 (Proposed)
**Status**: Design Phase
