# Data Cache Implementation

## Problem Statement

The agent was unable to follow the intended workflow:
1. Fetch full data
2. Aggregate it with chosen strategy

**Why it failed**: The `SmartTool` base class applies automatic preprocessing and truncation to tool results BEFORE the agent sees them. When `get_datasets(aggregation_level="full")` returned 102,920 tokens, it was truncated to 201 tokens via emergency summarization, leaving the agent with no data to aggregate.

## Solution: Data Cache Pattern

Implemented a **cache-based architecture** where large data is stored temporarily and referenced by cache keys instead of passed directly through tool results.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER: "Which datasets are available? I want a detailed list" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AGENT ITERATION 1: Fetch Data                                â”‚
â”‚                                                                â”‚
â”‚  Tool Call: get_datasets(aggregation_level="full")            â”‚
â”‚                                                                â”‚
â”‚  Backend:                                                      â”‚
â”‚    1. Fetch all 167 datasets from MetaData API               â”‚
â”‚    2. Store in cache: cache.store(data, key="datasets_full") â”‚
â”‚    3. Return metadata + cache_key (NOT the full data):       â”‚
â”‚       {                                                        â”‚
â”‚         "total": 167,                                          â”‚
â”‚         "cache_key": "datasets_full",                         â”‚
â”‚         "message": "Full data stored in cache",               â”‚
â”‚         "next_step": "Use aggregate_data with cache_key...",  â”‚
â”‚         "sample": [<first 2 items>]                           â”‚
â”‚       }                                                        â”‚
â”‚                                                                â”‚
â”‚  Token Count: ~500 tokens (not 102,920!)                     â”‚
â”‚  âœ“ No truncation needed - data is in cache                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AGENT ITERATION 2: Aggregate Data                            â”‚
â”‚                                                                â”‚
â”‚  Agent Decision: Call aggregate_data with cache_key           â”‚
â”‚                                                                â”‚
â”‚  Tool Call: aggregate_data(                                   â”‚
â”‚    strategy="extract_fields",                                 â”‚
â”‚    cache_key="datasets_full",                                 â”‚
â”‚    fields=["Shortname", "ApiDescription", "Dataspace"]       â”‚
â”‚  )                                                             â”‚
â”‚                                                                â”‚
â”‚  Backend:                                                      â”‚
â”‚    1. Load data from cache: cache.get("datasets_full")       â”‚
â”‚    2. Extract only requested fields from 167 datasets         â”‚
â”‚    3. Return reduced result (~10k tokens):                    â”‚
â”‚       {                                                        â”‚
â”‚         "strategy": "extract_fields",                          â”‚
â”‚         "original_count": 167,                                 â”‚
â”‚         "items": [                                             â”‚
â”‚           {"Shortname": "...", "ApiDescription": {...}}, ...  â”‚
â”‚         ]                                                      â”‚
â”‚       }                                                        â”‚
â”‚                                                                â”‚
â”‚  âœ“ Fits in token limits with only needed fields              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AGENT ITERATION 3: Respond to User                           â”‚
â”‚                                                                â”‚
â”‚  Agent has all information needed to answer the question      â”‚
â”‚  Returns detailed list with descriptions to user              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation Details

### 1. Data Cache Module (`backend/tools/data_cache.py`)

Created a simple in-memory cache with TTL (time-to-live):

```python
class DataCache:
    """In-memory cache for storing large data between tool calls"""

    def store(self, data, key=None) -> str:
        """Store data and return cache key"""

    def get(self, key: str) -> Any | None:
        """Retrieve data by key"""

    def delete(self, key: str) -> bool:
        """Delete cached data"""
```

**Features**:
- 5-minute TTL (auto-cleanup of expired entries)
- Access counting (tracks how many times data is retrieved)
- Automatic expiration
- Statistics tracking

### 2. Modified `get_datasets` Tool (`backend/tools/content_api.py`)

When `aggregation_level="full"`:

**Before**:
```python
return {"total": len(datasets), "datasets": datasets}
# Result: 102,920 tokens â†’ emergency truncation â†’ agent gets nothing
```

**After**:
```python
cache_key = cache.store(
    data={"total": len(datasets), "datasets": datasets},
    key="datasets_full"
)

return {
    "total": len(datasets),
    "cache_key": cache_key,
    "message": "Full dataset metadata stored in cache",
    "next_step": "Use aggregate_data tool with cache_key='datasets_full'",
    "sample": datasets[:2]  # Show 2 samples for context
}
# Result: ~500 tokens â†’ no truncation needed!
```

### 3. Modified `aggregate_data` Tool (`backend/tools/aggregation.py`)

Added `cache_key` parameter as alternative to `data`:

**Signature**:
```python
async def _aggregate_data(
    strategy: str,
    data: dict | list | None = None,
    cache_key: str | None = None,  # NEW!
    group_by: str | None = None,
    fields: list[str] | None = None,
    limit: int = 100,
    **kwargs
) -> dict:
```

**Logic**:
```python
if cache_key:
    logger.info(f"ğŸ“¦ Loading data from cache: {cache_key}")
    data = cache.get(cache_key)
    if data is None:
        return {"error": f"Cache key '{cache_key}' not found or expired"}
    logger.info(f"âœ“ Retrieved data from cache")

if data is None:
    return {"error": "Either 'data' or 'cache_key' parameter is required"}

# Then proceed with aggregation strategies...
```

## Logging Enhancements

Added extensive logging to track cache operations:

```
ğŸ“‹ Fetching datasets with aggregation_level='full', dataspace_filter=None
   Retrieved 167 datasets from MetaData API
   âš ï¸  FULL data requested (167 datasets) - storing in cache!
ğŸ’¾ Stored data in cache with key: datasets_full
   âœ… Result: 543 chars

ğŸ¤– AGENT ITERATION 2
ğŸ”§ AGENT DECISION: Call 1 tool(s)
   1. aggregate_data({'strategy': 'extract_fields', 'cache_key': 'datasets_full', ...})

âš™ï¸  EXECUTING TOOLS
â–¶ï¸  Tool 1/1: aggregate_data
ğŸ”„ AGGREGATION START: strategy='extract_fields', cache_key=datasets_full
   ğŸ“¦ Loading data from cache: datasets_full
   âœ“ Retrieved data from cache
ğŸ“Š Processing 167 items with strategy 'extract_fields'
âœ“ Extracted 3 fields from 167 items
âœ… AGGREGATION COMPLETE
```

## Benefits

### 1. âœ… Bypasses Token Limits
- Large data never passes through tool results
- Cache key (~30 chars) instead of data (~100k chars)
- No emergency truncation triggered

### 2. âœ… Agent Has Full Control
- Agent sees the cache_key and next_step instructions
- Agent can choose aggregation strategy based on user question
- Agent can call aggregate_data multiple times with different strategies

### 3. âœ… Efficient Memory Usage
- Data stored once, reused multiple times
- Automatic cleanup after 5 minutes (TTL)
- Can aggregate same data multiple ways without re-fetching

### 4. âœ… Clear Workflow
- Tool descriptions explicitly guide agent to use cache_key
- "next_step" field tells agent what to do
- Sample data provides context without bulk

### 5. âœ… Observable
- Extensive logs show cache operations
- Easy to debug: see when data stored/retrieved
- Access counting shows cache efficiency

## Tool Usage Examples

### Example 1: Detailed List

**User**: "Which datasets are available? I want a detailed list"

**Agent Workflow**:
```python
# Iteration 1: Fetch and cache
get_datasets(aggregation_level="full")
â†’ Returns: {cache_key: "datasets_full", total: 167, next_step: "..."}

# Iteration 2: Aggregate from cache
aggregate_data(
    strategy="extract_fields",
    cache_key="datasets_full",
    fields=["Shortname", "ApiDescription", "Dataspace", "ApiType"]
)
â†’ Returns: {items: [{Shortname: "...", ApiDescription: {...}}, ...]}

# Iteration 3: Respond
"Here are all 167 datasets with descriptions: ..."
```

### Example 2: Count by Type

**User**: "How many datasets are there of each type?"

**Agent Workflow**:
```python
# Iteration 1: Fetch and cache
get_datasets(aggregation_level="full")
â†’ Returns: {cache_key: "datasets_full", total: 167}

# Iteration 2: Count by type
aggregate_data(
    strategy="count_by",
    cache_key="datasets_full",
    group_by="ApiType"
)
â†’ Returns: {counts: {"content": 142, "timeseries": 25}}

# Iteration 3: Respond
"There are 142 content datasets and 25 timeseries datasets."
```

### Example 3: Multiple Aggregations

**User**: "Tell me about the datasets - types, dataspaces, and show some examples"

**Agent Workflow**:
```python
# Iteration 1: Fetch and cache
get_datasets(aggregation_level="full")
â†’ cache_key: "datasets_full"

# Iteration 2: Count by type
aggregate_data(strategy="count_by", cache_key="datasets_full", group_by="ApiType")
â†’ {"content": 142, "timeseries": 25}

# Iteration 3: Group by dataspace
aggregate_data(strategy="group_by", cache_key="datasets_full", group_by="Dataspace")
â†’ {"tourism": {count: 120, sample: [...]}, "mobility": {...}}

# Iteration 4: Get samples
aggregate_data(strategy="sample", cache_key="datasets_full", limit=5)
â†’ {items: [<5 full dataset objects>]}

# Iteration 5: Respond with comprehensive answer
```

## Configuration

### Environment Variables

```bash
# No new config needed - cache uses defaults
```

### Cache Settings (in code)

```python
# In data_cache.py
DataCache(ttl_minutes=5)  # 5-minute TTL

# Cache key for datasets
key="datasets_full"  # Fixed key for dataset metadata
```

## Troubleshooting

### Issue: "Cache key not found or expired"

**Cause**:
- Cache TTL expired (> 5 minutes since get_datasets was called)
- Agent used wrong cache key

**Fix**:
- Call `get_datasets(aggregation_level="full")` again to refresh cache
- Check logs for cache_key value

### Issue: Agent not using cache_key

**Symptoms**:
```
ğŸ”§ AGENT DECISION: Call 1 tool(s)
   1. aggregate_data({'strategy': 'extract_fields', 'data': <large_object>})
   âŒ No cache_key parameter!
```

**Fix**:
- Tool descriptions clearly state to use cache_key
- Agent should see "next_step" instruction in get_datasets response
- May need to adjust system prompt if agent consistently ignores instructions

### Issue: Cache memory usage

**Mitigation**:
- 5-minute TTL auto-cleans old entries
- Only one cache entry per session (key="datasets_full" is overwritten)
- Monitor with `cache.stats()` if needed

## Comparison: Before vs After

### Before (Failed Workflow)

```
User: "Detailed list please"
â†’ get_datasets(aggregation_level="full")
â†’ Returns 102,920 tokens
â†’ SmartTool truncates to 201 tokens (emergency summarization)
â†’ Agent receives: {"type": "object", "keys": ["total", "datasets"]}
â†’ Agent can't aggregate - has no data!
â†’ Agent makes up answer or asks user to retry
```

### After (Cache Workflow)

```
User: "Detailed list please"
â†’ get_datasets(aggregation_level="full")
â†’ Stores data in cache, returns cache_key
â†’ Agent receives: {cache_key: "datasets_full", next_step: "...", sample: [...]}
â†’ Agent calls: aggregate_data(strategy="extract_fields", cache_key="datasets_full", fields=[...])
â†’ Returns reduced data (~10k tokens) with only requested fields
â†’ Agent answers with complete, accurate information
```

## Future Enhancements

1. **Persistent Cache**: Use Redis instead of in-memory for multi-instance deployments
2. **Cache Warming**: Pre-cache common queries on startup
3. **Smart Key Management**: Generate unique keys per session/user
4. **Cache Statistics**: Expose cache hit rate, size metrics
5. **Configurable TTL**: Make TTL configurable per cache entry
6. **Auto-Cleanup**: Background task to cleanup expired entries

## Summary

The cache-based architecture solves the fundamental problem: **large data cannot pass through tool results due to token limits**.

By storing data in a temporary cache and passing only cache keys through tool results, we enable the agent to:
- Fetch full data without truncation
- Aggregate it in subsequent calls with chosen strategies
- Reuse the same data for multiple aggregations
- Follow the intended inspect-then-aggregate workflow

This pattern can be applied to other tools beyond `get_datasets` whenever large API responses need agent-controlled aggregation.

---

**Last Updated**: 2025-10-19
**Version**: 1.0
**Author**: OdhDiscovery Team
