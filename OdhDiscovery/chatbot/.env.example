# ODH Chatbot Configuration
# Copy this file to .env and fill in your values

# ===== LLM Provider Configuration =====
# Provider: togetherai, openai, anthropic, or custom
LLM_PROVIDER=togetherai

# Model identifier for your chosen provider
LLM_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo

# API key for your LLM provider
TOGETHER_API_KEY=your_api_key_here
# LLM_API_KEY=your_api_key_here  # Use this for openai/anthropic

# Temperature (0.0-2.0, lower = more focused)
LLM_TEMPERATURE=0.1

# Maximum tokens for responses
LLM_MAX_TOKENS=4096

# Custom base URL (for custom providers)
# LLM_BASE_URL=https://api.together.xyz/v1

# ===== API Configuration =====
# Content API base URL
# Note: The client appends endpoints like /MetaData or /Accommodation
# Examples:
#   - Via proxy (Docker): http://proxy:5000/api/v1/content
#   - Direct access: https://tourism.opendatahub.com/v1
#   - Local proxy: http://localhost:5000/api/v1/content
CONTENT_API_BASE_URL=http://proxy:5000/api/v1/content

# Timeseries API base URL (via proxy service)
TIMESERIES_API_BASE_URL=http://proxy:5000/api/v1/timeseries

# API request timeout in seconds
API_TIMEOUT=30

# ===== Vector Store Configuration =====
# ChromaDB host (use 'localhost' when running outside Docker)
CHROMA_HOST=chromadb

# ChromaDB port
CHROMA_PORT=8000

# Collection name for documentation
CHROMA_COLLECTION=odh_docs

# ===== Backend Configuration =====
# Backend server port
BACKEND_PORT=8001

# Backend server host
BACKEND_HOST=0.0.0.0

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable CORS
ENABLE_CORS=true

# ===== Preprocessing Configuration =====
# Maximum tokens per tool response
MAX_TOKENS_PER_TOOL=2000

# Enable automatic aggregation for large responses
ENABLE_AUTO_AGGREGATION=true

# Default page size for API calls
DEFAULT_PAGE_SIZE=50

# Maximum allowed page size
MAX_PAGE_SIZE=200
